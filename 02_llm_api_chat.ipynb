{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e29d54",
   "metadata": {},
   "source": [
    "# Part 2: Basic LLM Chat Tool\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this part, you'll create a simple command-line chat tool that interacts with a Large Language Model (LLM) through the Hugging Face API. This tool will allow you to have conversations with an LLM about healthcare topics.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Connect to the Hugging Face API\n",
    "- Create a basic interactive chat loop\n",
    "- Handle simple error cases\n",
    "- Test with healthcare questions\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a1b4ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root directory: /Users/hteshome/Desktop/7-transformers-haile-teshome\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict\n",
    "import openai\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(\"llm_test_logger\")\n",
    "\n",
    "project_root = Path.cwd()\n",
    "\n",
    "utils_dir = project_root / \"utils\"\n",
    "results_dir = project_root / \"results\" / \"part_2\"\n",
    "\n",
    "utils_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cfec5d",
   "metadata": {},
   "source": [
    "## 1. Connecting to the Hugging Face API\n",
    "\n",
    "The Hugging Face Inference API provides access to many language models. We'll use models that are available on the free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29017cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def query(message, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Send a prompt to the OpenAI ChatGPT API.\n",
    "\n",
    "    Args:\n",
    "        message (str): A string containing the user's input.\n",
    "        model (str): The model to query (default: gpt-3.5-turbo).\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": message}]\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"[Error] {str(e)}\"\n",
    "\n",
    "# Interactive test with graceful exit\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ChatGPT Chat — type 'exit' to quit.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nYou: \")\n",
    "                if user_input.lower() == \"exit\":\n",
    "                    print(\"Goodbye!\")\n",
    "                    break\n",
    "\n",
    "                response = query(user_input)\n",
    "                print(f\"ChatGPT: {response}\")\n",
    "\n",
    "            except EOFError:\n",
    "                print(\"\\n[Session ended with Ctrl+D]\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n[Interrupted with Ctrl+C — exiting]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b9fd2e",
   "metadata": {},
   "source": [
    "## 2. Creating Simple Chat Scripts\n",
    "\n",
    "Your task is to create two simple scripts that interact with the Hugging Face API:\n",
    "\n",
    "1. A basic one-off chat script (`utils/one_off_chat.py`)\n",
    "2. A contextual conversation script (`utils/conversation.py`)\n",
    "\n",
    "### One-Off Chat Script\n",
    "\n",
    "Create a script that handles independent interactions (each prompt/response is separate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d4d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def query(message, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Send a prompt to the OpenAI ChatGPT API.\n",
    "\n",
    "    Args:\n",
    "        message (str): A string containing the user's input.\n",
    "        model (str): The model to query (default: gpt-3.5-turbo).\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": message}]\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"[Error] {str(e)}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ChatGPT Test — type 'exit' to quit.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nYou: \")\n",
    "                if user_input.lower() == \"exit\":\n",
    "                    print(\"Goodbye!\")\n",
    "                    break\n",
    "\n",
    "                response = query(user_input)\n",
    "                print(f\"ChatGPT: {response}\")\n",
    "\n",
    "            except EOFError:\n",
    "                print(\"\\n[Session ended with Ctrl+D]\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n[Interrupted with Ctrl+C — exiting]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba67e56",
   "metadata": {},
   "source": [
    "### Contextual Conversation Script\n",
    "\n",
    "Create a script that maintains conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44aac2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Contextual LLM Chat! Type 'exit' to quit.\n",
      "ChatGPT: I am here to assist you with any questions or tasks you may have. Just let me know how I can help.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_response(prompt, history=None, model_name=\"gpt-3.5-turbo\", history_length=3):\n",
    "    \"\"\"\n",
    "    Get a response from the OpenAI model using conversation history\n",
    "    \n",
    "    Args:\n",
    "        prompt: The current user prompt\n",
    "        history: List of previous (prompt, response) tuples\n",
    "        model_name: Model to use (default: gpt-3.5-turbo)\n",
    "        history_length: How many past interactions to include\n",
    "        \n",
    "    Returns:\n",
    "        response_text: The model's response\n",
    "    \"\"\"\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # Build messages with past history \n",
    "    messages = []\n",
    "    for user, assistant in history[-history_length:]:\n",
    "        messages.append({\"role\": \"user\", \"content\": user})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "\n",
    "    # Add the current prompt\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_name,\n",
    "            messages=messages\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR] {e}\"\n",
    "\n",
    "def run_chat(model_name=\"gpt-3.5-turbo\", history_length=3):\n",
    "    \"\"\"\n",
    "    Run an interactive contextual chat session\n",
    "    \"\"\"\n",
    "    print(\"Contextual LLM Chat\\nType 'exit' to quit.\")\n",
    "    history = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nYou: \")\n",
    "            except EOFError:\n",
    "                print(\"\\n[EOF received] Exiting...\")\n",
    "                break\n",
    "\n",
    "            if user_input.lower().strip() == 'exit':\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "\n",
    "            response = get_response(user_input, history, model_name, history_length)\n",
    "            print(f\"ChatGPT: {response}\")\n",
    "\n",
    "            # Append current interaction to history\n",
    "            history.append((user_input, response))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n[Interrupted] Exiting...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_chat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fed1b8",
   "metadata": {},
   "source": [
    "## 3. Testing and Evaluation\n",
    "\n",
    "Create a script to test your chat implementations with specific healthcare questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ca4aaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: What are the symptoms of gout?\n",
      "Testing: How is gout diagnosed?\n",
      "Testing: What treatments are available for gout?\n",
      "Testing: What lifestyle changes can help manage gout?\n",
      "Testing: What foods should be avoided with gout?\n",
      "Test results saved to results/part_2/example.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd() / \"utils\"))\n",
    "\n",
    "from one_off_chat import query as get_one_off_response\n",
    "\n",
    "def test_chat(questions, model_name=\"gpt-3.5-turbo\", api_key=None):\n",
    "    if api_key:\n",
    "        import openai\n",
    "        openai.api_key = api_key\n",
    "\n",
    "    results = {}\n",
    "    for question in questions:\n",
    "        print(f\"Testing: {question}\")\n",
    "        response = get_one_off_response(message=question, model=model_name)\n",
    "        results[question] = response\n",
    "    return results\n",
    "\n",
    "test_questions = [\n",
    "    \"What are the symptoms of gout?\",\n",
    "    \"How is gout diagnosed?\",\n",
    "    \"What treatments are available for gout?\",\n",
    "    \"What lifestyle changes can help manage gout?\",\n",
    "    \"What foods should be avoided with gout?\"\n",
    "]\n",
    "\n",
    "def save_results(results, output_file=\"/Users/hteshome/Desktop/7-transformers-haile-teshome/results/part_2/example.txt\"):\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# LLM Chat Tool Test Results\\n\\n\")\n",
    "        f.write(\"## Usage Examples\\n\\n\")\n",
    "        f.write(\"```bash\\n\")\n",
    "        f.write(\"python /Users/hteshome/Desktop/7-transformers-haile-teshome/utils/one_off_chat.py --api_key YOUR_KEY\\n\")\n",
    "        f.write(\"python /Users/hteshome/Desktop/7-transformers-haile-teshome/utils/conversation.py --api_key YOUR_KEY\\n\")\n",
    "        f.write(\"```\\n\\n\")\n",
    "        f.write(\"## Test Results\\n\\n\")\n",
    "        f.write(\"```csv\\n\")\n",
    "        f.write(\"question,response\\n\")\n",
    "        for question, response in results.items():\n",
    "            q = question.replace(\",\", \" \").replace(\"\\n\", \" \").strip()\n",
    "            r = response.replace(\",\", \" \").replace(\"\\n\", \" \").strip()\n",
    "            f.write(f\"{q},{r}\\n\")\n",
    "        f.write(\"```\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\") or input(\"Enter your OpenAI API key: \").strip()\n",
    "    results = test_chat(test_questions, api_key=api_key)\n",
    "    save_results(results)\n",
    "    print(\"Test results saved to results/part_2/example.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93812ee",
   "metadata": {},
   "source": [
    "## Progress Checkpoints\n",
    "\n",
    "1. **API Connection**:\n",
    "   - [ ] Successfully connect to the Hugging Face API\n",
    "   - [ ] Send a query and receive a response\n",
    "   - [ ] Handle API errors gracefully\n",
    "\n",
    "2. **Chat Function Implementation**:\n",
    "   - [ ] Implement the get_response function\n",
    "   - [ ] Create the run_chat function for interactive sessions\n",
    "   - [ ] Handle errors and edge cases\n",
    "\n",
    "3. **Command Line Interface**:\n",
    "   - [ ] Create a parser with appropriate arguments\n",
    "   - [ ] Implement the main function\n",
    "   - [ ] Test the CLI functionality\n",
    "\n",
    "4. **Testing and Evaluation**:\n",
    "   - [ ] Test the functions with healthcare questions\n",
    "   - [ ] Save the results in a structured format\n",
    "   - [ ] Analyze the quality of responses\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "1. **API Access Issues**:\n",
    "   - Problem: Rate limiting\n",
    "   - Solution: Implement exponential backoff and retry logic\n",
    "   - Problem: Authentication errors\n",
    "   - Solution: Verify API key and environment variables\n",
    "\n",
    "2. **Response Parsing Issues**:\n",
    "   - Problem: Unexpected response format\n",
    "   - Solution: Add error handling for different response structures\n",
    "   - Problem: Empty or error responses\n",
    "   - Solution: Provide meaningful fallback responses\n",
    "\n",
    "3. **CLI Issues**:\n",
    "   - Problem: Arguments not parsed correctly\n",
    "   - Solution: Test with different argument combinations\n",
    "   - Problem: Script not executable\n",
    "   - Solution: Check file permissions\n",
    "\n",
    "## What to Submit\n",
    "\n",
    "1. Your implementation of the chat scripts:\n",
    "   - Basic requirement: `utils/one_off_chat.py` for single prompt/response chat\n",
    "   - Stretch goal (optional): `utils/conversation.py` for contextual chat\n",
    "   - Testing script: `utils/test_chat.py` to evaluate your implementation\n",
    "\n",
    "2. Test results in `results/part_2/example.txt` with the following format:\n",
    "   - Usage examples section showing how to run your scripts\n",
    "   - Test results section with CSV-formatted question/response pairs\n",
    "   - If you implemented the stretch goal, include examples of contextual exchanges\n",
    "\n",
    "The auto-grader should check:\n",
    "1. That your chat scripts can be executed\n",
    "2. That they correctly handle the test questions\n",
    "3. That your results file contains the required sections"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
